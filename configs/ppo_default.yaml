# PPO Training Configuration for Agent-RLlib
# Optimized for customer support and multi-agent scenarios

# Environment Configuration
env: "SupportBotEnv"
env_config:
  difficulty: "medium"
  tools: ["search", "calculator", "weather", "calendar"]
  max_turns: 20
  reward_shaping: true
  curriculum_level: 0.5
  render_mode: null

# Algorithm Configuration
algorithm: "PPO"
framework: "torch"

# Model Architecture
model:
  custom_model: "HybridPPOModel"
  custom_model_config:
    # LLM Integration
    llm_model: "gpt-3.5-turbo"
    llm_guidance_weight: 0.3
    
    # Neural Network Architecture
    hidden_dims: [512, 256, 128]
    use_attention: true
    dropout_rate: 0.1
    temperature: 0.8
    max_reasoning_steps: 5
    
    # Feature Processing
    text_embedding_dim: 512
    metadata_dim: 10
    
  # RLlib Model Settings
  fcnet_hiddens: [512, 256]
  fcnet_activation: "relu"
  use_lstm: false
  lstm_cell_size: 256
  lstm_use_prev_action: true
  lstm_use_prev_reward: true

# PPO Hyperparameters
# Learning Rate Schedule
lr: 3.0e-4
lr_schedule: 
  - [0, 3.0e-4]
  - [1000000, 1.0e-4]
  - [2000000, 3.0e-5]

# Training Batch Configuration
train_batch_size: 4096
sgd_minibatch_size: 128
num_sgd_iter: 10

# PPO-specific Parameters
gamma: 0.99
lambda: 0.95
clip_param: 0.2
vf_clip_param: 10.0
vf_loss_coeff: 0.5
entropy_coeff: 0.01
entropy_coeff_schedule:
  - [0, 0.01]
  - [500000, 0.005]
  - [1000000, 0.001]

# KL Divergence
kl_coeff: 0.2
kl_target: 0.01

# Value Function
use_critic: true
use_gae: true
vf_share_layers: false

# Rollout Configuration
rollout_fragment_length: 200
batch_mode: "truncate_episodes"

# Training Configuration
num_workers: 4
num_envs_per_worker: 1
num_cpus_per_worker: 1
num_gpus_per_worker: 0
num_gpus: 1

# Evaluation
evaluation_interval: 50
evaluation_duration: 10
evaluation_num_workers: 2
evaluation_config:
  env_config:
    difficulty: "hard"
    curriculum_level: 1.0
    render_mode: "human"
  explore: false

# Exploration
exploration_config:
  type: "EpsilonGreedy"
  initial_epsilon: 0.1
  final_epsilon: 0.02
  epsilon_timesteps: 100000

# Checkpointing and Logging
checkpoint_freq: 100
keep_checkpoints_num: 5
checkpoint_score_attr: "episode_reward_mean"

# Experiment Tracking
wandb:
  project: "agent-rllib"
  group: "ppo-experiments"
  name: "ppo-supportbot-v1"
  tags: ["ppo", "supportbot", "llm-guidance"]

# Stopping Criteria
stop:
  training_iteration: 2000
  episode_reward_mean: 50.0
  timesteps_total: 2000000

# Resource Configuration
resources_per_trial:
  cpu: 8
  gpu: 1
  memory: 16000000000  # 16GB

# Debugging and Monitoring
log_level: "INFO"
log_sys_usage: true
metrics_smoothing_episodes: 100

# Multi-Agent Configuration (if using multi-agent environments)
multiagent:
  policies: {}
  policy_mapping_fn: null
  policies_to_train: null

# Custom Callbacks
callbacks: "src.agent_rllib.training.callbacks.CustomCallbacks"

# Preprocessing
preprocessor_pref: "deepmind"
observation_filter: "MeanStdFilter"
synchronize_filters: true
compress_observations: false

# Fault Tolerance
ignore_worker_failures: false
recreate_failed_workers: false
restart_failed_sub_environments: false
num_consecutive_worker_failures_tolerance: 3

# Advanced Settings
tf_session_args:
  intra_op_parallelism_threads: 8
  inter_op_parallelism_threads: 8
  gpu_options:
    allow_growth: true

# Memory Management
replay_buffer_num_slots: 1
replay_buffer_shards: 1

# Curriculum Learning
curriculum:
  enabled: true
  initial_difficulty: 0.1
  max_difficulty: 1.0
  progression_rate: 0.05
  success_threshold: 0.8
  evaluation_episodes: 10
